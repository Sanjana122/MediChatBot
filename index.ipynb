{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Disease', 'Symptoms'], dtype='object')\n",
      "                                      Disease  \\\n",
      "0                            Fungal infection   \n",
      "1                            Fungal infection   \n",
      "2                            Fungal infection   \n",
      "3                            Fungal infection   \n",
      "4                            Fungal infection   \n",
      "...                                       ...   \n",
      "4915  (vertigo) Paroymsal  Positional Vertigo   \n",
      "4916                                     Acne   \n",
      "4917                  Urinary tract infection   \n",
      "4918                                Psoriasis   \n",
      "4919                                 Impetigo   \n",
      "\n",
      "                                               Symptoms  \n",
      "0     itching,  skin_rash,  nodal_skin_eruptions,  d...  \n",
      "1      skin_rash,  nodal_skin_eruptions,  dischromic...  \n",
      "2     itching,  nodal_skin_eruptions,  dischromic _p...  \n",
      "3             itching,  skin_rash,  dischromic _patches  \n",
      "4            itching,  skin_rash,  nodal_skin_eruptions  \n",
      "...                                                 ...  \n",
      "4915   vomiting,  headache,  nausea,  spinning_movem...  \n",
      "4916   skin_rash,  pus_filled_pimples,  blackheads, ...  \n",
      "4917   burning_micturition,  bladder_discomfort,  fo...  \n",
      "4918   skin_rash,  joint_pain,  skin_peeling,  silve...  \n",
      "4919   skin_rash,  high_fever,  blister,  red_sore_a...  \n",
      "\n",
      "[4920 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#column renaming\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "df.rename(columns={df.columns[0]: 'Disease'}, inplace=True)\n",
    "new_df = pd.concat([df['Disease'], df.iloc[:, 1:].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)], axis=1)\n",
    "\n",
    "\n",
    "new_df.rename(columns={new_df.columns[1]: 'Symptoms'}, inplace=True)\n",
    "\n",
    "new_df.to_csv('updated_dataset.csv', index=False)\n",
    "print(new_df.columns)\n",
    "print(new_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'processed_dataset.csv' with lowercase 'Disease' and 'Symptoms' columns and duplicate rows removed has been updated.\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicate rows and lower case conversion\n",
    "import csv\n",
    "\n",
    "def convert_to_lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "# Read the CSV file and convert 'Disease' and 'Symptoms' columns to lowercase\n",
    "with open('updated_dataset.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    rows = list(csv_reader)\n",
    "\n",
    "for row in rows:\n",
    "    row[0] = convert_to_lowercase(row[0])\n",
    "    row[1] = convert_to_lowercase(row[1])\n",
    "\n",
    "# Remove duplicate rows based on the entire row\n",
    "unique_rows = []\n",
    "seen_rows = set()\n",
    "\n",
    "for row in rows:\n",
    "    # Convert the row to a tuple to make it hashable\n",
    "    row_tuple = tuple(row)\n",
    "    \n",
    "    if row_tuple not in seen_rows:\n",
    "        unique_rows.append(row)\n",
    "        seen_rows.add(row_tuple)\n",
    "\n",
    "# Write the updated dataset with lowercase conversion and duplicate rows removed\n",
    "with open('processed_dataset.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(unique_rows)\n",
    "\n",
    "print(\"CSV file 'processed_dataset.csv' with lowercase 'Disease' and 'Symptoms' columns and duplicate rows removed has been updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'tokenized_dataset.csv' with 'Symptoms' column tokenized has been created.\n"
     ]
    }
   ],
   "source": [
    "#Tokenisation based on delimiters like [,   _]\n",
    "rows = []\n",
    "with open('processed_dataset.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_symptoms(symptoms):\n",
    "    tokens = []\n",
    "    symptom = \"\"\n",
    "    for char in symptoms:\n",
    "        if char == ',' or char == ' ':\n",
    "            if symptom:\n",
    "                tokens.append(symptom)\n",
    "                symptom = \"\"\n",
    "        else:\n",
    "            symptom += char\n",
    "    if symptom:\n",
    "        tokens.append(symptom)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "    row[1] = tokenize_symptoms(row[1])\n",
    "\n",
    "with open('tokenized_dataset.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerows(rows)\n",
    "\n",
    "print(\"CSV file 'tokenized_dataset.csv' with 'Symptoms' column tokenized has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'preprocessed.csv' with cleaned Symptoms has been created.\n"
     ]
    }
   ],
   "source": [
    "#Stopword removal to preprocessed csv file\n",
    "all_stopwords = set([\n",
    "    \"i\", \"me\", \"we\", \"our\", \"ours\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    " \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    " \"theirs\", \"themselves\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
    " \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
    " \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"'of'\", \"at\", \"by\", \"for\", \"about\", \"against\", \"between\",\n",
    " \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    " \"over\", \"under\", \"again\", \"to\",\"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    " \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"own\", \"same\", \"so\",\n",
    " \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\",\n",
    " \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\",\n",
    " \"mightn\", \"mustn\", \"needn\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\",\n",
    " \"also\", \"always\", \"amazing\", \"anyone\", \"anything\", \"appreciate\", \"around\", \"back\", \n",
    " \"best\", \"better\", \"big\", \"come\", \"could\", \"day\", \"dear\", \"definitely\", \"done\" , \"easy\", \"enjoy\", \"even\",\n",
    " \"ever\",\"every\", \"everyone\", \"everything\", \"excellent\", \"feel\", \"felt\", \"first\", \n",
    " \"free\", \"fun\", \"get\", \"getting\", \"good\", \"got\", \"great\", \"guy\", \"happy\", \"hard\",\n",
    " \"hear\", \"hello\", \"help\", \"hi\", \"hope\", \"idea\", \"keep\", \"kind\", \"know\", \"l\",\n",
    " \"large\", \"largely\", \"last\", \"later\", \"latest\", \"less\", \"lets\", \"long\", \"longer\",\n",
    " \"longest\", \"member\", \"members\", \"from\",\n",
    " \"mostly\", \"mr\", \"mrs\", \"n\", \"necessary\", \"need\", \"needed\", \"needing\", \"needs\",\n",
    " \"new\", \"newer\", \"newest\" ,\"nobody\", \"non\", \"none\", \"nowhere\",\n",
    " \"number\", \"numbers\", \"old\", \"older\", \"oldest\", \"open\", \"opened\",\n",
    " \"opening\", \"opens\", \"order\", \"ordered\", \"ordering\", \"orders\", \"others\", \"p\",\n",
    " \"parted\", \"parting\", \"parts\", \"per\", \"perhaps\", \"places\", \"point\", \"pointed\", \"pointing\", \"points\", \"possible\",\n",
    " \"present\", \"presented\", \"presenting\", \"presents\", \"problem\", \"problems\", \"puts\", \"q\", \"r\", \n",
    " \"room\", \"rooms\", \"saw\" , \"second\", \"seconds\", \"seemed\",\n",
    " \"seeming\", \"sees\", \"several\", \"shall\", \"showed\", \"showing\", \"shows\", \"side\", \"sides\", \n",
    " \"small\", \"smaller\", \"smallest\", \"somewhere\", \"state\", \"states\", \"taken\",\"therefore\",\n",
    " \"thinks\", \"this\", \"though\", \"thoughts\", \"three\", \n",
    " \"thus\", \"took\", \"toward\", \"turn\", \"turned\", \"turning\", \"turns\", \"u\", \n",
    " \"upon\", \"used\", \"uses\", \"v\", \"w\", \"wanted\", \"wanting\", \"way\",\n",
    " \"ways\", \"wells\", \"went\",\"whether\", \"whole\",\n",
    " \"whose\", \"with\", \"within\", \"work\", \"worked\", \"working\", \"works\", \"x\", \"years\",\n",
    " \"young\", \"younger\", \"youngest\",\"z\",\"zero\",\"on\",\"and\",\"of\",\n",
    "])\n",
    "\n",
    "import ast  \n",
    "\n",
    "def remove_stopwords(words):\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in all_stopwords:\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words\n",
    "\n",
    "\n",
    "input_file = \"tokenized_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    with open(input_file, \"r\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)\n",
    "\n",
    "        output_file = \"preprocessed.csv\"\n",
    "        with open(output_file, \"w\", newline=\"\") as output_csv_file:\n",
    "            writer = csv.writer(output_csv_file)\n",
    "            writer.writerow(header)\n",
    "\n",
    "            for row in reader:\n",
    "                if len(row) >= 2:\n",
    "                    disease = row[0]\n",
    "\n",
    "                    # Parse the string representation of the list into a Python list\n",
    "                    symptoms_str = row[1]\n",
    "                    symptoms_list = ast.literal_eval(symptoms_str)\n",
    "\n",
    "                    # Remove stopwords from the list of symptoms\n",
    "                    cleaned_symptoms = remove_stopwords(symptoms_list)\n",
    "\n",
    "                    \n",
    "                    cleaned_symptoms_str = \", \".join(cleaned_symptoms)\n",
    "\n",
    "                    writer.writerow([disease, cleaned_symptoms_str])\n",
    "                else:\n",
    "                    print(\"Skipping row with insufficient data:\", row)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found:\", input_file)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "print(f\"CSV file '{output_file}' with cleaned Symptoms has been created.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed data saved to 'stemmed_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "#Stemming suffix removal \n",
    "suffixes = [\"ing\",\n",
    "             \"ed\",\n",
    "            \"er\", \n",
    "            \"ness\", \n",
    "            \"s\", \n",
    "            \"es\", \n",
    "            \"ious\", \n",
    "            \"ment\", \n",
    "            \"able\", \n",
    "            \"ible\", \n",
    "            \"ize\", \n",
    "            \"ise\", \n",
    "            \"tion\", \n",
    "            \"al\", \n",
    "            \"ish\"]\n",
    "def stem(word):\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "input_file = \"preprocessed.csv\"\n",
    "output_file = \"stemmed_dataset.csv\"\n",
    "stemmed_data = []\n",
    "with open(input_file, \"r\", newline=\"\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    for row in reader:\n",
    "        disease_name = row[0]\n",
    "        symptoms = row[1].strip(\"[]\").replace(\"'\", \"\").split(\", \")\n",
    "        stemmed_disease_name = stem(disease_name)\n",
    "        stemmed_symptoms = [stem(symptom) for symptom in symptoms]\n",
    "        stemmed_data.append([stemmed_disease_name, stemmed_symptoms])\n",
    "\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Stemmed Disease\", \"Stemmed Symptoms\"])\n",
    "    writer.writerows(stemmed_data)\n",
    "\n",
    "print(f\"Stemmed data saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized data saved to 'lemmatized_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "#lemmatization \n",
    "lemmatization_dict = {\n",
    "    \"dischromic\": \"dischromia\",\n",
    "    \"sneez\": \"sneeze\",\n",
    "    \"urination\": \"urinate\",\n",
    "    \"indigestion\": \"indigest\",\n",
    "    \"concentration\": \"concentrate\",\n",
    "    \"irritability\": \"irritate\",\n",
    "    \"constipation\": \"constipate\",\n",
    "    \"eruption\": \"erupt\", \n",
    "    \"fungal infec\": \"fungus\",\n",
    "    \"continuou\" :\"continue\",\n",
    "    \"irritation\":\"irritate\",\n",
    "    \n",
    "}\n",
    "\n",
    "input_file = \"stemmed_dataset.csv\"\n",
    "output_file = \"lemmatized_dataset.csv\"\n",
    "\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    return lemmatization_dict.get(word, word)  \n",
    "\n",
    "\n",
    "with open(input_file, \"r\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader) \n",
    "\n",
    "    \n",
    "    with open(output_file, \"w\", newline=\"\") as output_csv_file:\n",
    "        writer = csv.writer(output_csv_file)\n",
    "        writer.writerow(header) \n",
    "\n",
    "        for row in reader:\n",
    "           \n",
    "            disease = lemmatize_word(row[0])\n",
    "\n",
    "            \n",
    "            symptoms_str = row[1].strip().replace(\"'\", \"\") \n",
    "            symptoms = symptoms_str.split(\", \")\n",
    "            lemmatized_symptoms = [lemmatize_word(symptom) for symptom in symptoms]\n",
    "\n",
    "            writer.writerow([disease, \", \".join(lemmatized_symptoms)])\n",
    "\n",
    "print(f\"Lemmatized data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import re\n",
    "\n",
    "# # Define a regular expression pattern for sentence segmentation.\n",
    "# sentence_splitter = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "# # Open the CSV file and create an output file for segmented data.\n",
    "# with open('lemmatized_dataset.csv', 'r') as input_file, open('segmented_dataset.csv', 'w', newline='') as output_file:\n",
    "#     reader = csv.reader(input_file)\n",
    "#     writer = csv.writer(output_file)\n",
    "\n",
    "#     for row in reader:\n",
    "#         # Assuming the lemmatized text is in the first column of the CSV.\n",
    "#         lemmatized_text = row[0]\n",
    "\n",
    "#         # Split the lemmatized text into sentences using the regular expression pattern.\n",
    "#         sentences = sentence_splitter.split(lemmatized_text)\n",
    "\n",
    "#         # Write the segmented sentences to the output CSV file.\n",
    "#         writer.writerow(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# class Word2Vec:\n",
    "#     def __init__(self, text_data, vector_size=100, window=5, min_count=1, sg=0):\n",
    "#         self.vector_size = vector_size\n",
    "#         self.window = window\n",
    "#         self.min_count = min_count\n",
    "#         self.sg = sg\n",
    "\n",
    "#         # Build a vocabulary of words.\n",
    "#         self.vocabulary = {}\n",
    "#         for text in text_data:\n",
    "#             for word in text:\n",
    "#                 if word not in self.vocabulary:\n",
    "#                     self.vocabulary[word] = 0\n",
    "#                 self.vocabulary[word] += 1\n",
    "\n",
    "#         # Remove words that occur less than the minimum count.\n",
    "#         for word in list(self.vocabulary.keys()):\n",
    "#             if self.vocabulary[word] < self.min_count:\n",
    "#                 del self.vocabulary[word]\n",
    "\n",
    "#         # Initialize the word embedding matrix.\n",
    "#         self.embeddings = np.random.randn(len(self.vocabulary), vector_size)\n",
    "\n",
    "#         # Train the Word2Vec model.\n",
    "#         self.train(text_data)\n",
    "\n",
    "#     def train(self, text_data):\n",
    "#         # Create a context window for each word in the text data.\n",
    "#         context_windows = []\n",
    "#         for text in text_data:\n",
    "#             for i in range(len(text)):\n",
    "#                 context_window = []\n",
    "#                 for j in range(i - self.window, i + self.window + 1):\n",
    "#                     if j < 0 or j >= len(text):\n",
    "#                         continue\n",
    "#                     if j != i:\n",
    "#                         context_window.append(text[j])\n",
    "#                 context_windows.append((text[i], context_window))\n",
    "\n",
    "#         # Update the word embedding matrix for each context window.\n",
    "#         for word, context_window in context_windows:\n",
    "#             word_embedding = self.embeddings[self.vocabulary[word]]\n",
    "#             context_embeddings = [self.embeddings[self.vocabulary[context_word]] for context_word in context_window]\n",
    "\n",
    "#             # Skip-gram model.\n",
    "#             if self.sg == 0:\n",
    "#                 for context_embedding in context_embeddings:\n",
    "#                     self.update_word_embedding(word_embedding, context_embedding)\n",
    "\n",
    "#             # Continuous Bag-of-Words (CBOW) model.\n",
    "#             else:\n",
    "#                 context_embedding = np.mean(context_embeddings, axis=0)\n",
    "#                 self.update_word_embedding(word_embedding, context_embedding)\n",
    "\n",
    "#     def update_word_embedding(self, word_embedding, context_embedding):\n",
    "#         # Calculate the error between the word embedding and the context embedding.\n",
    "#         error = word_embedding - context_embedding\n",
    "\n",
    "#         # Update the word embedding using the error.\n",
    "#         word_embedding += error * 0.01\n",
    "\n",
    "#     def get_word_embedding(self, word):\n",
    "#         return self.embeddings[self.vocabulary[word]]\n",
    "\n",
    "#     def most_similar(self, word, topn=10):\n",
    "#         word_embedding = self.get_word_embedding(word)\n",
    "\n",
    "#         # Calculate the cosine similarity between the word embedding and all other word embeddings.\n",
    "#         similarities = np.dot(self.embeddings, word_embedding)\n",
    "\n",
    "#         # Sort the similarities in descending order.\n",
    "#         sorted_similarities = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#         # Return the top n most similar words.\n",
    "#         return [self.vocabulary[i] for i, _ in sorted_similarities[:topn]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''import numpy as np\n",
    "import re\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, text_data, vector_size=100, window=5, min_count=1, sg=0):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "\n",
    "        # Build a vocabulary of words.\n",
    "        self.vocabulary = {}\n",
    "        for text in text_data:\n",
    "            for word in text:\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = 0\n",
    "                self.vocabulary[word] += 1\n",
    "\n",
    "        # Remove words that occur less than the minimum count.\n",
    "        for word in list(self.vocabulary.keys()):\n",
    "            if self.vocabulary[word] < self.min_count:\n",
    "                del self.vocabulary[word]\n",
    "\n",
    "        # Initialize the word embedding matrix.\n",
    "        self.embeddings = np.random.randn(len(self.vocabulary), vector_size)\n",
    "\n",
    "        # Train the Word2Vec model.\n",
    "        self.train(text_data)\n",
    "\n",
    "    def train(self, text_data):\n",
    "        # Create a context window for each word in the text data.\n",
    "        context_windows = []\n",
    "        for text in text_data:\n",
    "            for i in range(len(text)):\n",
    "                context_window = []\n",
    "                for j in range(i - self.window, i + self.window + 1):\n",
    "                    if j < 0 or j >= len(text):\n",
    "                        continue\n",
    "                    if j != i:\n",
    "                        context_window.append(text[j])\n",
    "                context_windows.append((text[i], context_window))\n",
    "\n",
    "        # Update the word embedding matrix for each context window.\n",
    "        for word, context_window in context_windows:\n",
    "            word_embedding = self.embeddings[self.vocabulary[word]]\n",
    "            context_embeddings = [self.embeddings[self.vocabulary[context_word]] for context_word in context_window]\n",
    "\n",
    "            # Skip-gram model.\n",
    "            if self.sg == 0:\n",
    "                for context_embedding in context_embeddings:\n",
    "                    self.update_word_embedding(word_embedding, context_embedding)\n",
    "\n",
    "            # Continuous Bag-of-Words (CBOW) model.\n",
    "            else:\n",
    "                context_embedding = np.mean(context_embeddings, axis=0)\n",
    "                self.update_word_embedding(word_embedding, context_embedding)\n",
    "\n",
    "    def update_word_embedding(self, word_embedding, context_embedding):\n",
    "        # Calculate the error between the word embedding and the context embedding.\n",
    "        error = word_embedding - context_embedding\n",
    "\n",
    "        # Update the word embedding using the error.\n",
    "        word_embedding += error * 0.01\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[self.vocabulary[word]]\n",
    "\n",
    "    def most_similar(self, word, topn=10):\n",
    "        word_embedding = self.get_word_embedding(word)\n",
    "\n",
    "        # Calculate the cosine similarity between the word embedding and all other word embeddings.\n",
    "        similarities = np.dot(self.embeddings, word_embedding)\n",
    "\n",
    "        # Sort the similarities in descending order.\n",
    "        sorted_similarities = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top n most similar words.\n",
    "        return [self.vocabulary[i] for i, _ in sorted_similarities[:topn]]\n",
    "\n",
    "\n",
    "# Get user input for text_data\n",
    "user_input = input(\"Enter your query: \")\n",
    "text_data = [user_input.split()]  # Convert the input into a list of words\n",
    "\n",
    "# Create an instance of the Word2Vec class\n",
    "word2vec_model = Word2Vec(text_data)\n",
    "\n",
    "# Save the word embeddings to a file\n",
    "np.savetxt('word_embeddings.txt', word2vec_model.embeddings, delimiter=',')\n",
    "\n",
    "# Test the get_word_embedding function\n",
    "word_embedding = word2vec_model.get_word_embedding(\"word2\")\n",
    "print(\"Word Embedding for 'word2':\", word_embedding)\n",
    "\n",
    "# Test the most_similar function\n",
    "similar_words = word2vec_model.most_similar(\"word1\")\n",
    "print(\"Most similar words to 'word1':\", similar_words)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized dataset has been written to: vectorized.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    return text\n",
    "\n",
    "def create_vocabulary(text_data):\n",
    "    words = [word for text in text_data for word in preprocess_text(text).split()]\n",
    "    return set(words)\n",
    "\n",
    "def vectorize_text(text, vocabulary):\n",
    "    vector = Counter(preprocess_text(text).split())\n",
    "    return [vector[word] for word in vocabulary]\n",
    "\n",
    "def read_csv(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        for line in csvfile:\n",
    "            entry = line.strip().split(',')\n",
    "            dataset.append(entry)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def write_csv(file_path, data, headers):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(headers)\n",
    "        for entry in data:\n",
    "            csv_writer.writerow(entry)\n",
    "\n",
    "# Read data from CSV file\n",
    "csv_file_path = 'tokenized_dataset.csv'  # Replace with the path to your CSV file\n",
    "dataset = read_csv(csv_file_path)\n",
    "\n",
    "# Extract diseases and symptoms separately\n",
    "diseases = [entry[0] for entry in dataset]\n",
    "symptoms = [\" \".join(entry[1:]) for entry in dataset]\n",
    "\n",
    "# Create vocabulary for diseases and symptoms\n",
    "disease_vocabulary = create_vocabulary(diseases)\n",
    "symptom_vocabulary = create_vocabulary(symptoms)\n",
    "\n",
    "# Vectorize diseases and symptoms\n",
    "vectorized_diseases = [vectorize_text(disease, disease_vocabulary) for disease in diseases]\n",
    "vectorized_symptoms = [vectorize_text(symptom, symptom_vocabulary) for symptom in symptoms]\n",
    "\n",
    "# Combine vectors and write to output CSV file\n",
    "output_headers = [\"Disease\"] + list(symptom_vocabulary)\n",
    "output_dataset = [[disease] + symptom_vector for disease, symptom_vector in zip(diseases, vectorized_symptoms)]\n",
    "output_file_path = 'vectorized.csv'  # Replace with the desired output file path\n",
    "write_csv(output_file_path, output_dataset, output_headers)\n",
    "\n",
    "print(f\"Vectorized dataset has been written to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized User Input: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# vectorizing user input\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    return text\n",
    "\n",
    "def vectorize_text(text, vocabulary):\n",
    "    vector = Counter(preprocess_text(text).split())\n",
    "    return [vector[word] for word in vocabulary]\n",
    "\n",
    "# Assuming you already have disease_vocabulary and symptom_vocabulary from previous code\n",
    "\n",
    "# Take user input\n",
    "user_input = \"i am suffering from fever,cold,running nose and headache\"\n",
    "\n",
    "# Vectorize user input\n",
    "vectorized_user_input = vectorize_text(user_input, symptom_vocabulary)\n",
    "\n",
    "# Print the vectorized user input\n",
    "print(\"Vectorized User Input:\", vectorized_user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import csv\n",
    "\n",
    "# Load your preprocessed and lemmatized text data\n",
    "with open('preprocessed.csv', 'r') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    text_data = [row[0] for row in reader]\n",
    "\n",
    "# Define a function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i + n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Generate and store n-grams for your text data\n",
    "n = 2  # You can choose the value of 'n' for your desired n-grams (e.g., 2 for bigrams)\n",
    "ngram_data = []\n",
    "for text in text_data:\n",
    "    ngrams = generate_ngrams(text, n)\n",
    "    ngram_data.append(ngrams)\n",
    "\n",
    "# At this point, 'ngram_data' contains the n-grams for each text in your dataset.\n",
    "\n",
    "# Now, you can use these n-grams for further analysis, such as creating phrase embeddings.'''\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text.lower())\n",
    "    return text\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    ngrams_list = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams_list.append(tokens[i:i+n])\n",
    "    return ngrams_list\n",
    "\n",
    "def process_csv(input_file, output_file, n):\n",
    "    with open(input_file, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        header = next(csv_reader)\n",
    "\n",
    "        # Find the column containing text data dynamically\n",
    "        text_column_index = None\n",
    "        for index, col in enumerate(header):\n",
    "            if 'description' in col.lower() or 'symptoms' in col.lower():\n",
    "                text_column_index = index\n",
    "                break\n",
    "\n",
    "        if text_column_index is None:\n",
    "            raise ValueError(\"Text column not found in CSV file.\")\n",
    "\n",
    "        # Read and process the CSV file\n",
    "        data = []\n",
    "        for row in csv_reader:\n",
    "            text = row[text_column_index]\n",
    "            tokens = preprocess_text(text).split()\n",
    "            ngrams_list = generate_ngrams(tokens, n)\n",
    "            data.append({'text': text, 'ngrams': ngrams_list})\n",
    "\n",
    "    # Write the results to an output CSV file\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['text', f'ngrams_{n}'])\n",
    "\n",
    "        for entry in data:\n",
    "            csv_writer.writerow([entry['text'], entry['ngrams']])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = 'tokenized_dataset.csv'  # Replace with the path to your CSV file\n",
    "output_csv_file = 'output_ngrams.csv'  # Replace with the desired output file path\n",
    "n_value = 2  # Change this to the desired n-gram value\n",
    "\n",
    "process_csv(input_csv_file, output_csv_file, n_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptoms: ['itching', 'skin_rash']\n",
      "Predicted Disease: migraine\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, training_data):\n",
    "        self.class_probabilities = collections.defaultdict(int)\n",
    "        self.feature_probabilities = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "\n",
    "        for row in training_data:\n",
    "            intent = row[0]  # Assuming the disease is in the first column\n",
    "            self.class_probabilities[intent] += 1\n",
    "\n",
    "            for i in range(1, len(row)):\n",
    "                word = row[i]\n",
    "                self.feature_probabilities[intent][word] += 1\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, csv_file):\n",
    "        training_data = []\n",
    "        with open(csv_file, 'r') as csvfile:\n",
    "            csv_reader = csv.reader(csvfile)\n",
    "            for row in csv_reader:\n",
    "                training_data.append(row)\n",
    "        return cls(training_data)\n",
    "\n",
    "    def classify_symptoms(self, symptoms):\n",
    "        probabilities = {}\n",
    "\n",
    "        for intent in self.class_probabilities:\n",
    "            # Calculate the prior probability of the intent\n",
    "            prior_probability = self.class_probabilities[intent] / len(self.class_probabilities)\n",
    "\n",
    "            # Calculate the likelihood of the symptoms given the intent\n",
    "            likelihood = 1.0\n",
    "            for i in range(1, len(symptoms) + 1):\n",
    "                word = symptoms[i - 1]\n",
    "                word_count = self.feature_probabilities[intent].get(word, 0) + 1\n",
    "                total_count = sum(self.feature_probabilities[intent].values()) + len(self.feature_probabilities)\n",
    "                word_probability = word_count / total_count\n",
    "                likelihood *= word_probability\n",
    "\n",
    "            # Multiply the prior and likelihood to get the overall probability of the intent\n",
    "            probability = prior_probability * likelihood\n",
    "\n",
    "            probabilities[intent] = probability\n",
    "\n",
    "        # Return the intent with the highest probability\n",
    "        predicted_intent = max(probabilities, key=probabilities.get)\n",
    "        return predicted_intent\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = 'tokenized_dataset.csv'  # Replace with the path to your CSV file\n",
    "classifier = NaiveBayesClassifier.from_csv(csv_file_path)\n",
    "symptoms = [\"itching\", \"skin_rash\"]  # Replace with the symptoms you want to classify\n",
    "predicted_disease = classifier.classify_symptoms(symptoms)\n",
    "\n",
    "print(f\"Symptoms: {symptoms}\")\n",
    "print(f\"Predicted Disease: {predicted_disease}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Entities in Most Similar Row:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the medical dataset\n",
    "# Replace 'processed_dataset.csv' with the path to your dataset\n",
    "df = pd.read_csv('./data/tokenized_dataset.csv')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", str(text).lower())\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the symptoms column\n",
    "df['[\\'symptoms\\']']= df['[\\'symptoms\\']'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['[\\'symptoms\\']'])  # Change to 'symptoms'\n",
    "\n",
    "# Function to load medical entity rules from a CSV file\n",
    "def load_rules_from_csv(csv_file):\n",
    "    rules = {}\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            entity_type = row[0]\n",
    "            pattern = ','.join(row[1:])\n",
    "            rules[entity_type] = re.compile(pattern)\n",
    "    return rules\n",
    "\n",
    "# Function to extract medical entities from text data using a rule-based approach\n",
    "def extract_medical_entities(text_data, rules):\n",
    "    # Convert the text data to a string\n",
    "    text_data = str(text_data)\n",
    "\n",
    "    # Initialize medical_entities list\n",
    "    medical_entities = []\n",
    "\n",
    "    # Extract medical entities from the text data\n",
    "    for rule_name, rule in rules.items():\n",
    "        for match in rule.finditer(text_data):\n",
    "            medical_entities.append(f\"{rule_name}: {match.group()}\")\n",
    "\n",
    "    # Return the medical entities\n",
    "    return medical_entities\n",
    "\n",
    "# Load entity rules from a CSV file\n",
    "entity_rules = load_rules_from_csv('data/vectorized_dataset.csv')  # Replace with your CSV file path\n",
    "\n",
    "# User input\n",
    "user_input = \"i am suffering from fever, cold, running nose, and headache\"\n",
    "\n",
    "# Vectorize user input using the same vectorizer used for symptoms\n",
    "vectorized_user_input = tfidf_vectorizer.transform([preprocess_text(user_input)])\n",
    "\n",
    "# Calculate cosine similarity between the user input and each row in the DataFrame\n",
    "cosine_similarities = cosine_similarity(vectorized_user_input, tfidf_matrix)\n",
    "most_similar_index = cosine_similarities[0].argsort()[-1]\n",
    "most_similar_row = df.iloc[most_similar_index]['[\\'symptoms\\']']\n",
    "\n",
    "# Extract medical entities from the most similar row\n",
    "most_similar_row_entities = extract_medical_entities(most_similar_row, entity_rules)\n",
    "\n",
    "# Print the medical entities\n",
    "print(\"Medical Entities in Most Similar Row:\")\n",
    "print(most_similar_row_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP\\ChatBot_postMid\\ChatBot_postMid\\ChatBot NLP final\\ChatBot NLP\\ChatBot\\index.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP/ChatBot_postMid/ChatBot_postMid/ChatBot%20NLP%20final/ChatBot%20NLP/ChatBot/index.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# TF-IDF Vectorization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP/ChatBot_postMid/ChatBot_postMid/ChatBot%20NLP%20final/ChatBot%20NLP/ChatBot/index.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m tfidf_vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NLP/ChatBot_postMid/ChatBot_postMid/ChatBot%20NLP%20final/ChatBot%20NLP/ChatBot/index.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39;49mfit_transform(df[\u001b[39m'\u001b[39;49m\u001b[39msymptoms\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP/ChatBot_postMid/ChatBot_postMid/ChatBot%20NLP%20final/ChatBot%20NLP/ChatBot/index.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39midentify_disease\u001b[39m(symptoms):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP/ChatBot_postMid/ChatBot_postMid/ChatBot%20NLP%20final/ChatBot%20NLP/ChatBot/index.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# Vectorize user input using the same vectorizer used for symptoms\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP/ChatBot_postMid/ChatBot_postMid/ChatBot%20NLP%20final/ChatBot%20NLP/ChatBot/index.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     vectorized_user_input \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mtransform([preprocess_text(symptoms)])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "class DialogueState:\n",
    "    def __init__(self):\n",
    "        self.current_goal = None\n",
    "        self.identified_entities = {}\n",
    "        self.previous_utterances = []\n",
    "\n",
    "    def set_current_goal(self, goal):\n",
    "        self.current_goal = goal\n",
    "\n",
    "    def add_identified_entity(self, entity_type, entity_value):\n",
    "        self.identified_entities[entity_type] = entity_value\n",
    "\n",
    "    def add_previous_utterance(self, utterance):\n",
    "        self.previous_utterances.append(utterance)\n",
    "\n",
    "    def get_current_goal(self):\n",
    "        return self.current_goal\n",
    "\n",
    "    def get_identified_entities(self):\n",
    "        return self.identified_entities\n",
    "\n",
    "    def get_previous_utterances(self):\n",
    "        return self.previous_utterances\n",
    "\n",
    "# Load the vectorized dataset\n",
    "df = pd.read_csv('vectorized.csv')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", str(text).lower())\n",
    "    return text\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['[\\'symptoms\\']'])\n",
    "\n",
    "def identify_disease(symptoms):\n",
    "    # Vectorize user input using the same vectorizer used for symptoms\n",
    "    vectorized_user_input = tfidf_vectorizer.transform([preprocess_text(symptoms)])\n",
    "\n",
    "    # Calculate cosine similarity between the user input and each row in the DataFrame\n",
    "    cosine_similarities = cosine_similarity(vectorized_user_input, tfidf_matrix)\n",
    "    most_similar_index = cosine_similarities[0].argsort()[-1]\n",
    "    identified_disease = df.iloc[most_similar_index]['disease']\n",
    "\n",
    "    return identified_disease\n",
    "\n",
    "# Example usage\n",
    "dialogue_state = DialogueState()\n",
    "\n",
    "# User input: Symptoms\n",
    "user_symptoms = \"fever, cough, headache\"\n",
    "\n",
    "# Identify disease\n",
    "identified_disease = identify_disease(user_symptoms)\n",
    "\n",
    "# Set the user's current goal\n",
    "dialogue_state.set_current_goal(\"identify_disease\")\n",
    "\n",
    "# Add the identified disease to the dialogue state\n",
    "dialogue_state.add_identified_entity(\"disease\", identified_disease)\n",
    "\n",
    "# Generate a response based on the user's goal and the identified disease\n",
    "response = f\"The identified disease based on symptoms is: {identified_disease}\"\n",
    "\n",
    "# Add the response to the dialogue state\n",
    "dialogue_state.add_previous_utterance(response)\n",
    "\n",
    "# Print the response to the user\n",
    "print(response)\n",
    "\n",
    "# Now, you can use the identified disease to look up medication information in your list\n",
    "# For simplicity, let's assume you have a dictionary mapping diseases to medications\n",
    "medications_for_diseases = {\n",
    "    \"fungal infection\": \"Antifungal medication\",\n",
    "    \"Acne\": \"Topical creams\",\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Get the identified disease\n",
    "identified_disease = dialogue_state.get_identified_entities()[\"disease\"]\n",
    "\n",
    "# Lookup medication for the disease\n",
    "medication_for_disease = medications_for_diseases.get(identified_disease, \"Medication information not available\")\n",
    "\n",
    "# Generate a response with medication information\n",
    "medication_response = f\"The medication for {identified_disease} is: {medication_for_disease}\"\n",
    "\n",
    "# Add the medication response to the dialogue state\n",
    "dialogue_state.add_previous_utterance(medication_response)\n",
    "\n",
    "# Print the medication response to the user\n",
    "print(medication_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy  # Make sure to install the 'spacy' library\n",
    "\n",
    "class DialogueState:\n",
    "    def __init__(self):\n",
    "        self.current_goal = None\n",
    "        self.identified_entities = {}\n",
    "        self.previous_utterances = []\n",
    "\n",
    "    def set_current_goal(self, goal):\n",
    "        self.current_goal = goal\n",
    "\n",
    "    def add_identified_entity(self, entity_type, entity_value):\n",
    "        self.identified_entities[entity_type] = entity_value\n",
    "\n",
    "    def add_previous_utterance(self, utterance):\n",
    "        self.previous_utterances.append(utterance)\n",
    "\n",
    "    def get_current_goal(self):\n",
    "        return self.current_goal\n",
    "\n",
    "    def get_identified_entities(self):\n",
    "        return self.identified_entities\n",
    "\n",
    "    def get_previous_utterances(self):\n",
    "        return self.previous_utterances\n",
    "\n",
    "class DialogueFSM:\n",
    "    def __init__(self):\n",
    "        # Define the states of the FSM.\n",
    "        self.states = {\n",
    "            \"start\": StartState(),\n",
    "            \"transition_to_next_state\": TransitionToNextState(),\n",
    "            \"identify_disease\": IdentifyDiseaseState(),\n",
    "            \"provide_medication\": ProvideMedicationState(),\n",
    "            \"end\": EndState()\n",
    "        }\n",
    "\n",
    "        # Set the current state.\n",
    "        self.current_state = \"start\"\n",
    "        self.dialogue_state = DialogueState()\n",
    "\n",
    "    def transition(self, input_utterance):\n",
    "        # Get the next state based on the current state and the input utterance.\n",
    "        next_state = self.states[self.current_state].get_next_state(input_utterance)\n",
    "\n",
    "        # Set the current state to the next state.\n",
    "        self.current_state = next_state\n",
    "\n",
    "        # Execute actions for the current state.\n",
    "        self.states[self.current_state].execute_actions(input_utterance, self.dialogue_state)\n",
    "\n",
    "\n",
    "class StartState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # Always transition to an intermediate state before moving to the actual state.\n",
    "        return \"transition_to_next_state\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Greet the user and provide instructions on how to use the system.\n",
    "        print(\"Welcome to the medical chatbot. I can help you find information about symptoms, diseases, and medications.\")\n",
    "        print(\"To get started, please tell me about your symptoms.\")\n",
    "\n",
    "class TransitionToNextState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # Add logic here to determine the next state based on the input.\n",
    "        # For example, if \"symptom\" is in the input, transition to \"identify_disease\".\n",
    "        if \"symptom\" in input_utterance:\n",
    "            return \"identify_disease\"\n",
    "        # If the input doesn't contain symptoms and it's not a specific command, transition to \"start\".\n",
    "        elif not any(cmd in input_utterance.lower() for cmd in [\"medication\", \"disease\"]):\n",
    "            return \"start\"\n",
    "        # Add more conditions as needed.\n",
    "        else:\n",
    "            return \"transition_to_next_state\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # No actions needed for this state.\n",
    "        pass\n",
    "\n",
    "\n",
    "class IdentifyDiseaseState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # After identifying the disease, transition to the provide_medication state.\n",
    "        return \"provide_medication\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Extract symptoms from the user's input\n",
    "        symptoms = extract_symptoms(input_utterance)\n",
    "\n",
    "        # Identify the disease based on symptoms (you might use a more advanced model for this).\n",
    "        identified_disease = identify_disease(symptoms)\n",
    "\n",
    "        # Add the identified disease and symptoms to the dialogue state.\n",
    "        dialogue_state.add_identified_entity(\"disease\", identified_disease)\n",
    "        dialogue_state.add_identified_entity(\"symptoms\", symptoms)\n",
    "\n",
    "        # Generate a response based on the identified disease.\n",
    "        response = f\"I believe you may have {identified_disease} based on the symptoms: {', '.join(symptoms)}. Let me provide information about the medication.\"\n",
    "\n",
    "        # Add the response to the dialogue state.\n",
    "        dialogue_state.add_previous_utterance(response)\n",
    "\n",
    "        # Print the response to the user.\n",
    "        print(response)\n",
    "\n",
    "class ProvideMedicationState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # Stay in the provide_medication state.\n",
    "        return \"provide_medication\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Get the identified disease from the dialogue state.\n",
    "        identified_disease = dialogue_state.get_identified_entities().get(\"disease\", \"unknown disease\")\n",
    "\n",
    "        # Lookup medication for the disease (you might have a more sophisticated lookup mechanism).\n",
    "        medication_for_disease = lookup_medication(identified_disease)\n",
    "\n",
    "        # Generate a response with medication information.\n",
    "        medication_response = f\"The medication for {identified_disease} is: {medication_for_disease}\"\n",
    "\n",
    "        # Add the medication response to the dialogue state.\n",
    "        dialogue_state.add_previous_utterance(medication_response)\n",
    "\n",
    "        # Print the medication response to the user.\n",
    "        print(medication_response)\n",
    "\n",
    "class EndState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # Stay in the end state.\n",
    "        return \"end\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Provide a closing message.\n",
    "        print(\"Thank you for using the medical chatbot. If you have more questions, feel free to ask.\")\n",
    "\n",
    "def identify_disease(symptoms):\n",
    "    # Placeholder function for identifying disease based on symptoms.\n",
    "    # In a real application, you might use a machine learning model or a database for this.\n",
    "    return \"ExampleDisease\"\n",
    "\n",
    "def lookup_medication(disease):\n",
    "    # Placeholder function for looking up medication based on the disease.\n",
    "    # In a real application, you might have a more sophisticated medication database.\n",
    "    return \"ExampleMedication\"\n",
    "\n",
    "# Add a new function to extract symptoms from user input using spaCy\n",
    "def extract_symptoms(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    symptoms = [ent.text for ent in doc.ents if ent.label_ == \"SYMPTOM\"]\n",
    "    return symptoms\n",
    "\n",
    "# Create a DialogueFSM object.\n",
    "dialog_fsm = DialogueFSM()\n",
    "\n",
    "# Process the user's input utterance.\n",
    "user_utterance_1 = \"I have a headache and fever.\"\n",
    "dialog_fsm.transition(user_utterance_1)\n",
    "\n",
    "# Process the user's next input utterance.\n",
    "user_utterance_2 = \"What could be the disease?\"\n",
    "dialog_fsm.transition(user_utterance_2)\n",
    "\n",
    "# Process the user's next input utterance.\n",
    "user_utterance_3 = \"Tell me about the medication for this disease.\"\n",
    "dialog_fsm.transition(user_utterance_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Jinja2==2.11.3 in c:\\users\\nikitha angel\\appdata\\roaming\\python\\python311\\site-packages (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nikitha angel\\appdata\\roaming\\python\\python311\\site-packages (from Jinja2==2.11.3) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Jinja2==2.11.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the medical chatbot. I can help you find information about symptoms, diseases, and medications.\n",
      "To get started, please tell me about your symptoms.\n",
      "I believe you may have ExampleDisease. Let me provide information about the medication.\n"
     ]
    }
   ],
   "source": [
    "class IdentifyDiseaseState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # After identifying the disease, transition to the provide_medication state.\n",
    "        return \"provide_medication\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Identify the disease based on symptoms (you might use a more advanced model for this).\n",
    "        identified_disease = identify_disease(input_utterance)\n",
    "\n",
    "        # Add the identified disease to the dialogue state.\n",
    "        dialogue_state.add_identified_entity(\"disease\", identified_disease)\n",
    "\n",
    "        # Generate a response based on the identified disease.\n",
    "        response = f\"I believe you may have {identified_disease}. Let me provide information about the medication.\"\n",
    "\n",
    "        # Add the response to the dialogue state.\n",
    "        dialogue_state.add_previous_utterance(response)\n",
    "\n",
    "        # Print the response to the user.\n",
    "        print(response)\n",
    "\n",
    "\n",
    "class ProvideMedicationState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # Stay in the provide_medication state.\n",
    "        return \"provide_medication\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Get the identified disease from the dialogue state.\n",
    "        identified_disease = dialogue_state.get_identified_entities().get(\"disease\", \"unknown disease\")\n",
    "\n",
    "        # Lookup medication for the disease (you might have a more sophisticated lookup mechanism).\n",
    "        medication_for_disease = lookup_medication(identified_disease)\n",
    "\n",
    "        # Generate a response with medication information.\n",
    "        medication_response = f\"The medication for {identified_disease} is: {medication_for_disease}\"\n",
    "\n",
    "        # Add the medication response to the dialogue state.\n",
    "        dialogue_state.add_previous_utterance(medication_response)\n",
    "\n",
    "        # Print the medication response to the user.\n",
    "        print(medication_response)\n",
    "\n",
    "\n",
    "class ProvideSymptomInformationState:\n",
    "    def get_next_state(self, input_utterance):\n",
    "        # Stay in the provide_symptom_information state.\n",
    "        return \"provide_symptom_information\"\n",
    "\n",
    "    def execute_actions(self, input_utterance, dialogue_state):\n",
    "        # Get information about the symptoms (you might have a more sophisticated lookup mechanism).\n",
    "        symptom_information = lookup_symptom_information(input_utterance)\n",
    "\n",
    "        # Generate a response with symptom information.\n",
    "        symptom_response = f\"Here is information about the symptoms: {symptom_information}\"\n",
    "\n",
    "        # Add the symptom response to the dialogue state.\n",
    "        dialogue_state.add_previous_utterance(symptom_response)\n",
    "\n",
    "        # Print the symptom response to the user.\n",
    "        print(symptom_response)\n",
    "\n",
    "\n",
    "# Placeholder function for looking up symptom information.\n",
    "def lookup_symptom_information(symptoms):\n",
    "    # In a real application, you might have a more sophisticated symptom information database.\n",
    "    return \"Example symptom information\"\n",
    "\n",
    "# Continue the code with the existing classes and functions\n",
    "\n",
    "# Example usage\n",
    "dialog_fsm = DialogueFSM()\n",
    "\n",
    "# Process the user's input utterance.\n",
    "user_utterance_1 = \"I have a headache and fever.\"\n",
    "dialog_fsm.transition(user_utterance_1)\n",
    "\n",
    "# Process the user's next input utterance.\n",
    "user_utterance_2 = \"What could be the disease?\"\n",
    "dialog_fsm.transition(user_utterance_2)\n",
    "\n",
    "# Process the user's next input utterance.\n",
    "user_utterance_3 = \"Tell me about the medication for this disease.\"\n",
    "dialog_fsm.transition(user_utterance_3)\n",
    "\n",
    "# Process the user's next input utterance.\n",
    "user_utterance_4 = \"What are the symptoms of the disease?\"\n",
    "dialog_fsm.transition(user_utterance_4)\n",
    "\n",
    "# Note: You can continue to extend the chatbot by adding more states and functionalities as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The symptoms of a heart attack can vary from person to person, but they often include chest pain, shortness of breath, nausea, and vomiting. If you are experiencing any of these symptoms, it is important to seek medical attention immediately.\n"
     ]
    }
   ],
   "source": [
    "def generate_nlm_response(basic_response):\n",
    "\n",
    "  \"\"\"Generates a natural language response using a neural language model.\n",
    "\n",
    "  Args:\n",
    "    basic_response: The basic response generated by the template.\n",
    "\n",
    "  Returns:\n",
    "    A refined response generated by the neural language model.\n",
    "  \"\"\"\n",
    "\n",
    "  # TODO: Implement this function to generate a response using a neural language model.\n",
    "\n",
    "  return basic_response\n",
    "\n",
    "# Get the basic response from the template.\n",
    "basic_response = \"The symptoms of a heart attack can vary from person to person, but they often include chest pain, shortness of breath, nausea, and vomiting.\"\n",
    "\n",
    "# Use the NLM to refine the response.\n",
    "nlm_response = generate_nlm_response(basic_response)\n",
    "\n",
    "# Combine the basic and NLM responses.\n",
    "final_response = \"{} If you are experiencing any of these symptoms, it is important to seek medical attention immediately.\".format(nlm_response)\n",
    "\n",
    "# Print the response to the user.\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intent_classification_accuracy(true_intents, predicted_intents):\n",
    "\n",
    "  \"\"\"Calculates the intent classification accuracy.\n",
    "\n",
    "  Args:\n",
    "    true_intents: A list of the true intents.\n",
    "    predicted_intents: A list of the predicted intents.\n",
    "\n",
    "  Returns:\n",
    "    The intent classification accuracy.\n",
    "  \"\"\"\n",
    "\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(true_intents)):\n",
    "    if true_intents[i] == predicted_intents[i]:\n",
    "      correct_predictions += 1\n",
    "\n",
    "  return correct_predictions / len(true_intents)\n",
    "\n",
    "def calculate_entity_recognition_f1_score(true_entities, predicted_entities):\n",
    "\n",
    "  \"\"\"Calculates the entity recognition F1-score.\n",
    "\n",
    "  Args:\n",
    "    true_entities: A list of the true entities.\n",
    "    predicted_entities: A list of the predicted entities.\n",
    "\n",
    "  Returns:\n",
    "    The entity recognition F1-score.\n",
    "  \"\"\"\n",
    "\n",
    "  from sklearn.metrics import f1_score\n",
    "\n",
    "  return f1_score(true_entities, predicted_entities, average=\"micro\")\n",
    "\n",
    "def calculate_question_answering_accuracy(true_answers, predicted_answers):\n",
    "\n",
    "  \"\"\"Calculates the question answering accuracy.\n",
    "\n",
    "  Args:\n",
    "    true_answers: A list of the true answers.\n",
    "    predicted_answers: A list of the predicted answers.\n",
    "\n",
    "  Returns:\n",
    "    The question answering accuracy.\n",
    "  \"\"\"\n",
    "\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(true_answers)):\n",
    "    if true_answers[i] == predicted_answers[i]:\n",
    "      correct_predictions += 1\n",
    "\n",
    "  return correct_predictions / len(true_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
